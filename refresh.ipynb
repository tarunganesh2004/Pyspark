{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "032cddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f34ae85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Test1\").getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba2de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|Sepal_Length|Sepal_Width|Petal_Length|Petal_Width|Species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2| setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2| setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1| setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1| setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2| setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4| setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4| setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3| setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3| setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv(\"iris/iris.csv\",header=True,inferSchema=True,sep=\",\")\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37499f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Sepal_Length: double, Sepal_Width: double, Petal_Length: double, Petal_Width: double, Species: string]\n"
     ]
    }
   ],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7fb929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-----+-------+-------+----+\n",
      "|  id|  name|gender|Maths|physics|english|java|\n",
      "+----+------+------+-----+-------+-------+----+\n",
      "|8955| Tarun|  Male|   78|     50|     45|  25|\n",
      "|8871|   Ali|  Male|  100|    100|     99|  98|\n",
      "|8892|  Rafi|  Male|  100|     91|    100|  99|\n",
      "|8912|  Uday|  Male|  100|     97|     99|  98|\n",
      "|8910|Random|Female|   80|     40|     60|  54|\n",
      "+----+------+------+-----+-------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from textfile(rdd to data frame)\n",
    "data = sc.textFile(\"students.txt\")\n",
    "\n",
    "data = data.map(lambda line: line.split(\",\"))\n",
    "data = data.map(lambda x:( x[0][:4], x[0][4:], x[1], *x[2].split(\":\")))\n",
    "data.collect()\n",
    "\n",
    "# convert to dataframe\n",
    "# can be also done using .toDF()\n",
    "# df2=spark.createDataFrame(data) # creates with default columns\n",
    "df2=spark.createDataFrame(data,[\"id\",\"name\",\"gender\",\"Maths\",\"physics\",\"english\",\"java\"])\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdfb734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- Maths: integer (nullable = true)\n",
      " |-- physics: integer (nullable = true)\n",
      " |-- english: integer (nullable = true)\n",
      " |-- java: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one more better option is use col function \n",
    "# col() is used to refer a column as an expression\n",
    "from pyspark.sql.functions import col\n",
    "df2=df2.select(\n",
    "    col(\"id\"),\n",
    "    col(\"name\"),\n",
    "    col(\"gender\"),\n",
    "    col(\"Maths\").cast(\"int\"),\n",
    "    col(\"physics\").cast(\"int\"),\n",
    "    col(\"english\").cast(\"int\"),\n",
    "    col(\"java\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# for simple selecting like df.name col is not used\n",
    "\n",
    "# while doing calculations/filtering/logic --> use col()\n",
    "\n",
    "#now its converted to proper datatypes\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2972920a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5.1, 3.5, 1.4, 0.2, 'setosa'),\n",
       " (4.9, 3.0, 1.4, 0.2, 'setosa'),\n",
       " (4.7, 3.2, 1.3, 0.2, 'setosa'),\n",
       " (4.6, 3.1, 1.5, 0.2, 'setosa'),\n",
       " (5.0, 3.6, 1.4, 0.2, 'setosa'),\n",
       " (5.4, 3.9, 1.7, 0.4, 'setosa'),\n",
       " (4.6, 3.4, 1.4, 0.3, 'setosa'),\n",
       " (5.0, 3.4, 1.5, 0.2, 'setosa'),\n",
       " (4.4, 2.9, 1.4, 0.2, 'setosa'),\n",
       " (4.9, 3.1, 1.5, 0.1, 'setosa')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df to rdd\n",
    "df1.rdd.map(tuple).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7516a5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'gender', 'Maths', 'physics', 'english', 'java']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes # all data types\n",
    "\n",
    "df2.columns # all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "779a7db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|  id|  name|\n",
      "+----+------+\n",
      "|8955| Tarun|\n",
      "|8871|   Ali|\n",
      "|8892|  Rafi|\n",
      "|8912|  Uday|\n",
      "|8910|Random|\n",
      "+----+------+\n",
      "\n",
      "+-----+\n",
      "|maths|\n",
      "+-----+\n",
      "|   78|\n",
      "|  100|\n",
      "|  100|\n",
      "|  100|\n",
      "|   80|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"id\",\"name\").show()\n",
    "# renaming column name\n",
    "df2.select(col(\"Maths\").alias(\"maths\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8304b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('CustomerId', IntegerType(), True), StructField('CustomerName', StringType(), True), StructField('CustomerLocation', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "# StructType (used to define the schema of the row)\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "\n",
    "empSchema=pyspark.sql.types.StructType([\n",
    "    StructField(\"CustomerId\",IntegerType(),True),\n",
    "StructField(\"CustomerName\",StringType(),True),\n",
    "StructField(\"CustomerLocation\",StringType(),True)\n",
    "])\n",
    "\n",
    "print(empSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f55e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|CustomerId|CustomerName|    CustomerLocation|\n",
      "+----------+------------+--------------------+\n",
      "|      1002|        Aman|     1 Anthes Avenue|\n",
      "|      1003|       Harsh|   87985 Linden Pass|\n",
      "|      1004|       Ayush| 56 La Follette Pass|\n",
      "|      1005|       Aditi|  8 Briar Crest Pass|\n",
      "|      1006|      Anjali|    035 Iowa Terrace|\n",
      "|      1007|     Shubham|    3925 Clove Drive|\n",
      "|      1008|     Anushka|    9 Straubel Drive|\n",
      "|      1009|       Rohit|   816 Northland Way|\n",
      "|      1010|     Saurabh|    10165 Gerald Way|\n",
      "|      1011|      Muskan|83 Merchant Junction|\n",
      "|      1012|       Rahul|1249 Summerview Pass|\n",
      "|      1013|     Utkarsh|   5 Bowman Junction|\n",
      "|      1014|     Vaibhav|      3 Chinook Park|\n",
      "|      1015|        Amit|   90535 Bonner Lane|\n",
      "|      1016|      Saumya| 056 Straubel Avenue|\n",
      "|      1017|     Rishabh| 70 Wayridge Parkway|\n",
      "|      1018|      Shruti|     609 Truax Alley|\n",
      "|      1019|    Himanshu|948 Marquette Circle|\n",
      "|      1020|       Kajal|  4121 Atwood Circle|\n",
      "|      1021|       Ankit|    31 Center Avenue|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('CustomerId', 'int'),\n",
       " ('CustomerName', 'string'),\n",
       " ('CustomerLocation', 'string')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assigning defined structure to dataframe\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "empSchema = pyspark.sql.types.StructType(\n",
    "    [\n",
    "        StructField(\"CustomerId\", IntegerType(), True),\n",
    "        StructField(\"CustomerName\", StringType(), True),\n",
    "        StructField(\"CustomerLocation\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .schema(empSchema)\n",
    "    .option(\"header\", True)\n",
    "    .load(\"Cust_address.csv\")\n",
    ")\n",
    "df.show()\n",
    "df.dtypes\n",
    "\n",
    "# df.drop(\"CustomerLocation\") # drop column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
