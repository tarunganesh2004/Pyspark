{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9651a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[1]\")  # IMPORTANT: single core\n",
    "    .appName(\"RDD Learning\")\n",
    "    .config(\"spark.hadoop.hadoop.native.io.disable\", \"true\")\n",
    "    .config(\"spark.python.worker.reuse\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "# for windows\n",
    "# .config(\"spark.hadoop.hadoop.native.io.disable\", \"true\") --> this is better (to store files)\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c7ef2",
   "metadata": {},
   "source": [
    "Handling Structured Data using RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1721a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['5.1', '3.5', '1.4', '0.2', 'setosa'], ['4.9', '3.0', '1.4', '0.2', 'setosa'], ['4.7', '3.2', '1.3', '0.2', 'setosa'], ['4.6', '3.1', '1.5', '0.2', 'setosa'], ['5.0', '3.6', '1.4', '0.2', 'setosa'], ['5.4', '3.9', '1.7', '0.4', 'setosa'], ['4.6', '3.4', '1.4', '0.3', 'setosa'], ['5.0', '3.4', '1.5', '0.2', 'setosa'], ['4.4', '2.9', '1.4', '0.2', 'setosa'], ['4.9', '3.1', '1.5', '0.1', 'setosa']]\n"
     ]
    }
   ],
   "source": [
    "# split based on delimiter\n",
    "iris1 = sc.textFile(\"iris/iris_site.csv\")\n",
    "\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "print(iris1_split.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb8684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['5.1', '3.5', '1.4', '0.2', 'setosa'],\n",
       " ['4.9', '3.0', '1.4', '0.2', 'setosa'],\n",
       " ['4.7', '3.2', '1.3', '0.2', 'setosa'],\n",
       " ['4.6', '3.1', '1.5', '0.2', 'setosa'],\n",
       " ['5.0', '3.6', '1.4', '0.2', 'setosa'],\n",
       " ['5.4', '3.9', '1.7', '0.4', 'setosa'],\n",
       " ['4.6', '3.4', '1.4', '0.3', 'setosa'],\n",
       " ['5.0', '3.4', '1.5', '0.2', 'setosa'],\n",
       " ['4.4', '2.9', '1.4', '0.2', 'setosa'],\n",
       " ['4.9', '3.1', '1.5', '0.1', 'setosa']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chaining spark commands\n",
    "# in spark multiple transformation and action can be combined by chaining\n",
    "# them with dot operator\n",
    "sc.textFile(\"iris/iris_site.csv\").map(lambda var1: var1.split(\",\")).take(10)\n",
    "\n",
    "\n",
    "# defining transformation(instead of lambda functions we can use normal defined functions)\n",
    "def fun1(var1):\n",
    "    return var1.split(\",\")\n",
    "\n",
    "\n",
    "sc.textFile(\"iris/iris_site.csv\").map(fun1).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efacf43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5.1', '4.9', '4.7', '4.6', '5.0', '5.4', '4.6', '5.0', '4.4', '4.9']\n"
     ]
    }
   ],
   "source": [
    "# Extracting a particular column\n",
    "iris1 = sc.textFile(\"iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "\n",
    "iris1_mod = iris1_split.map(lambda col: col[0])  # only 1st column\n",
    "print(iris1_mod.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31f52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3.0, 3.0, 4.0, 4.4, 3.9, 3.5, 3.8, 3.8, 3.4, 3.7, 3.6, 3.3, 3.4, 3.0, 3.4, 3.5, 3.4, 3.2, 3.1, 3.4, 4.1, 4.2, 3.1, 3.2, 3.5, 3.6, 3.0, 3.4, 3.5, 2.3, 3.2, 3.5, 3.8, 3.0, 3.8, 3.2, 3.7, 3.3, 3.2, 3.2, 3.1, 2.3, 2.8, 2.8, 3.3, 2.4, 2.9, 2.7, 2.0, 3.0, 2.2, 2.9, 2.9, 3.1, 3.0, 2.7, 2.2, 2.5, 3.2, 2.8, 2.5, 2.8, 2.9, 3.0, 2.8, 3.0, 2.9, 2.6, 2.4, 2.4, 2.7, 2.7, 3.0, 3.4, 3.1, 2.3, 3.0, 2.5, 2.6, 3.0, 2.6, 2.3, 2.7, 3.0, 2.9, 2.9, 2.5, 2.8, 3.3, 2.7, 3.0, 2.9, 3.0, 3.0, 2.5, 2.9, 2.5, 3.6, 3.2, 2.7, 3.0, 2.5, 2.8, 3.2, 3.0, 3.8, 2.6, 2.2, 3.2, 2.8, 2.8, 2.7, 3.3, 3.2, 2.8, 3.0, 2.8, 3.0, 2.8, 3.8, 2.8, 2.8, 2.6, 3.0, 3.4, 3.1, 3.0, 3.1, 3.1, 3.1, 2.7, 3.2, 3.3, 3.0, 2.5, 3.0, 3.4, 3.0]\n"
     ]
    }
   ],
   "source": [
    "# type casting\n",
    "iris1 = sc.textFile(\"iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "\n",
    "type_casting = iris1_split.map(lambda col: float(col[1]))\n",
    "print(type_casting.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d1ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5 <class 'float'>\n",
      "3.0 <class 'float'>\n",
      "3.2 <class 'float'>\n",
      "3.1 <class 'float'>\n",
      "3.6 <class 'float'>\n",
      "3.9 <class 'float'>\n",
      "3.4 <class 'float'>\n",
      "3.4 <class 'float'>\n",
      "2.9 <class 'float'>\n",
      "3.1 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# for loop with rdd objects\n",
    "# type_casting=iris1_split.map(lambda col:float(col[1]))\n",
    "for var1 in type_casting.take(10):\n",
    "    print(var1, type(var1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2845aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1, 3.5, 1.4, 0.2, 'setosa'], [4.9, 3.0, 1.4, 0.2, 'setosa'], [4.7, 3.2, 1.3, 0.2, 'setosa'], [4.6, 3.1, 1.5, 0.2, 'setosa'], [5.0, 3.6, 1.4, 0.2, 'setosa'], [5.4, 3.9, 1.7, 0.4, 'setosa'], [4.6, 3.4, 1.4, 0.3, 'setosa'], [5.0, 3.4, 1.5, 0.2, 'setosa'], [4.4, 2.9, 1.4, 0.2, 'setosa'], [4.9, 3.1, 1.5, 0.1, 'setosa']]\n"
     ]
    }
   ],
   "source": [
    "# typecasting multiple columns\n",
    "iris_split_mod = iris1_split.map(\n",
    "    lambda var1: [\n",
    "        float(var1[0]),\n",
    "        float(var1[1]),\n",
    "        float(var1[2]),\n",
    "        float(var1[3]),\n",
    "        var1[4],\n",
    "    ]\n",
    ")\n",
    "print(iris_split_mod.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6312fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Sepal.Length', 5.1), ('Sepal.Width', 3.5), ('Petal.Length', 1.4), ('Petal.Width', 0.2)), (('Sepal.Length', 4.9), ('Sepal.Width', 3.0), ('Petal.Length', 1.4), ('Petal.Width', 0.2)), (('Sepal.Length', 4.7), ('Sepal.Width', 3.2), ('Petal.Length', 1.3), ('Petal.Width', 0.2)), (('Sepal.Length', 4.6), ('Sepal.Width', 3.1), ('Petal.Length', 1.5), ('Petal.Width', 0.2)), (('Sepal.Length', 5.0), ('Sepal.Width', 3.6), ('Petal.Length', 1.4), ('Petal.Width', 0.2)), (('Sepal.Length', 5.4), ('Sepal.Width', 3.9), ('Petal.Length', 1.7), ('Petal.Width', 0.4)), (('Sepal.Length', 4.6), ('Sepal.Width', 3.4), ('Petal.Length', 1.4), ('Petal.Width', 0.3)), (('Sepal.Length', 5.0), ('Sepal.Width', 3.4), ('Petal.Length', 1.5), ('Petal.Width', 0.2)), (('Sepal.Length', 4.4), ('Sepal.Width', 2.9), ('Petal.Length', 1.4), ('Petal.Width', 0.2)), (('Sepal.Length', 4.9), ('Sepal.Width', 3.1), ('Petal.Length', 1.5), ('Petal.Width', 0.1))]\n"
     ]
    }
   ],
   "source": [
    "# creating key value pairs in RDD\n",
    "iris1_mod = iris1_split.map(\n",
    "    lambda var1: (\n",
    "        (\"Sepal.Length\", float(var1[0])),\n",
    "        (\"Sepal.Width\", float(var1[1])),\n",
    "        (\"Petal.Length\", float(var1[2])),\n",
    "        (\"Petal.Width\", float(var1[3])),\n",
    "    )\n",
    ")  # by using this the entire key value collection is present inside a double collection\n",
    "# to remove this extra level we use flatMap function instead of map\n",
    "print(iris1_mod.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb2b3201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sepal.Length', 5.1), ('Sepal.Width', 3.5), ('Petal.Length', 1.4), ('Petal.Width', 0.2), ('Sepal.Length', 4.9), ('Sepal.Width', 3.0), ('Petal.Length', 1.4), ('Petal.Width', 0.2), ('Sepal.Length', 4.7), ('Sepal.Width', 3.2)]\n"
     ]
    }
   ],
   "source": [
    "iris1_mod = iris1_split.flatMap(\n",
    "    lambda var1: (\n",
    "        (\"Sepal.Length\", float(var1[0])),\n",
    "        (\"Sepal.Width\", float(var1[1])),\n",
    "        (\"Petal.Length\", float(var1[2])),\n",
    "        (\"Petal.Width\", float(var1[3])),\n",
    "    )\n",
    ")\n",
    "print(iris1_mod.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb280bc",
   "metadata": {},
   "source": [
    "## Sorting RDD\n",
    "\n",
    "sorted based on a particular column using sortBy function\n",
    "\n",
    "sortByKey --> sorting rdd based on key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce1ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris1 = sc.textFile(\"iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))  # this is common for all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c8703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.3, 3.0, 1.1, 0.1, 'setosa'),\n",
       " (4.4, 2.9, 1.4, 0.2, 'setosa'),\n",
       " (4.4, 3.0, 1.3, 0.2, 'setosa'),\n",
       " (4.4, 3.2, 1.3, 0.2, 'setosa'),\n",
       " (4.5, 2.3, 1.3, 0.3, 'setosa'),\n",
       " (4.6, 3.1, 1.5, 0.2, 'setosa'),\n",
       " (4.6, 3.4, 1.4, 0.3, 'setosa'),\n",
       " (4.6, 3.6, 1.0, 0.2, 'setosa'),\n",
       " (4.6, 3.2, 1.4, 0.2, 'setosa'),\n",
       " (4.7, 3.2, 1.3, 0.2, 'setosa')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple sort\n",
    "iris1_mod = iris1_split.map(\n",
    "    lambda var1: (\n",
    "        float(var1[0]),\n",
    "        float(var1[1]),\n",
    "        float(var1[2]),\n",
    "        float(var1[3]),\n",
    "        var1[4],\n",
    "    )\n",
    ")  # type convert all string columns\n",
    "\n",
    "iris1_mod.sortBy(lambda x: x[0]).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab7ed89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('setosa', ['5.1', 3.5, 1.4, 0.2]),\n",
       " ('setosa', ['4.9', 3.0, 1.4, 0.2]),\n",
       " ('setosa', ['4.7', 3.2, 1.3, 0.2]),\n",
       " ('setosa', ['4.6', 3.1, 1.5, 0.2]),\n",
       " ('setosa', ['5.0', 3.6, 1.4, 0.2]),\n",
       " ('setosa', ['5.4', 3.9, 1.7, 0.4]),\n",
       " ('setosa', ['4.6', 3.4, 1.4, 0.3]),\n",
       " ('setosa', ['5.0', 3.4, 1.5, 0.2]),\n",
       " ('setosa', ['4.4', 2.9, 1.4, 0.2]),\n",
       " ('setosa', ['4.9', 3.1, 1.5, 0.1])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorting RDD based on key\n",
    "iris1_mod = iris1_split.map(\n",
    "    lambda var1: (var1[4], [var1[0], float(var1[1]), float(var1[2]), float(var1[3])])\n",
    ")\n",
    "\n",
    "iris1_mod.sortByKey(ascending=True, keyfunc=lambda k: k).take(\n",
    "    10\n",
    ")  # defaultly its sorts based on key only, so lambda is unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d7cf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sepal.Length', 5.1], ['Sepal.Length', 4.9], ['Sepal.Length', 4.7], ['Sepal.Length', 4.6], ['Sepal.Length', 5.0], ['Sepal.Length', 5.4], ['Sepal.Length', 4.6], ['Sepal.Length', 5.0], ['Sepal.Length', 4.4], ['Sepal.Length', 4.9]]\n",
      "[['Sepal.Width', 3.5], ['Sepal.Width', 3.0], ['Sepal.Width', 3.2], ['Sepal.Width', 3.1], ['Sepal.Width', 3.6], ['Sepal.Width', 3.9], ['Sepal.Width', 3.4], ['Sepal.Width', 3.4], ['Sepal.Width', 2.9], ['Sepal.Width', 3.1]]\n",
      "[['Petal.Length', 1.4], ['Petal.Length', 1.4], ['Petal.Length', 1.3], ['Petal.Length', 1.5], ['Petal.Length', 1.4], ['Petal.Length', 1.7], ['Petal.Length', 1.4], ['Petal.Length', 1.5], ['Petal.Length', 1.4], ['Petal.Length', 1.5]]\n",
      "[['Petal.Width', 0.2], ['Petal.Width', 0.2], ['Petal.Width', 0.2], ['Petal.Width', 0.2], ['Petal.Width', 0.2], ['Petal.Width', 0.4], ['Petal.Width', 0.3], ['Petal.Width', 0.2], ['Petal.Width', 0.2], ['Petal.Width', 0.1]]\n",
      "[['Sepal.Length', 5.1], ['Sepal.Length', 4.9], ['Sepal.Length', 4.7], ['Sepal.Length', 4.6], ['Sepal.Length', 5.0], ['Sepal.Length', 5.4], ['Sepal.Length', 4.6], ['Sepal.Length', 5.0], ['Sepal.Length', 4.4], ['Sepal.Length', 4.9]]\n"
     ]
    }
   ],
   "source": [
    "# Union Multiple RDD's\n",
    "SL = iris1_split.map(lambda var1: [\"Sepal.Length\", float(var1[0])])\n",
    "SW = iris1_split.map(lambda var1: [\"Sepal.Width\", float(var1[1])])\n",
    "PL = iris1_split.map(lambda var1: [\"Petal.Length\", float(var1[2])])\n",
    "PW = iris1_split.map(lambda var1: [\"Petal.Width\", float(var1[3])])\n",
    "CV = iris1_split.map(lambda var1: [\"Species\", var1[4]])\n",
    "\n",
    "print(SL.take(10))\n",
    "print(SW.take(10))\n",
    "print(PL.take(10))\n",
    "print(PW.take(10))\n",
    "\n",
    "union_data = sc.union([SL, SW, PL, PW])\n",
    "print(union_data.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersection (used to find the common elements of two RDD's)\n",
    "r1 = sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "r2 = sc.parallelize([\"d\", \"e\", \"f\", \"g\"])\n",
    "# print(r1.intersection(r2).collect()) works in python 3.10 or 3.9 version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed4de2",
   "metadata": {},
   "source": [
    "Joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1f53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('employee1', ([3, 1, 2, 5, 4], [4, 5, 2, 1, 1])), ('employee2', ([2, 4, 1, 2, 2], [2, 1, 1, 1, 2]))]\n"
     ]
    }
   ],
   "source": [
    "# Joins\n",
    "#  Two rdds containing data in the form of key-value pair can be joined\n",
    "# by using join function\n",
    "location1 = sc.parallelize(\n",
    "    [(\"employee1\", [3, 1, 2, 5, 4]), (\"employee2\", [2, 4, 1, 2, 2])]\n",
    ")\n",
    "location2 = sc.parallelize(\n",
    "    [(\"employee2\", [2, 1, 1, 1, 2]), (\"employee1\", [4, 5, 2, 1, 1])]\n",
    ")\n",
    "\n",
    "join1 = location1.join(location2)\n",
    "print(join1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc9916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['employee1', [4, 5, 2, 1, 1]], ['employee2', [2, 1, 1, 1, 2]]]\n"
     ]
    }
   ],
   "source": [
    "# extract column values of second RDD\n",
    "print(join1.map(lambda var1: [var1[0], var1[1][1]]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd837d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('282', ['339', '20070401', '20070413', '20070408', '523', '282', '1', '3', 2.0, 469.794, 0.0, 0.0, 486.7066, 973.4132, 939.588, 23.4897]), ('282', ['213', '20060801', '20060813', '20060808', '403', '282', '1', '4', 2.0, 20.1865, 0.0, 0.0, 13.8782, 27.7564, 40.373, 1.0093]), ('282', ['586', '20070701', '20070713', '20070708', '205', '282', '13', '4', 13.0, 334.0575, 0.15, 651.4121, 461.4448, 5998.7824, 3691.3354, 92.2834]), ('282', ['459', '20060801', '20060813', '20060808', '79', '282', '1', '3', 5.0, 53.994, 0.0, 0.0, 37.1209, 185.6045, 269.97, 6.7493])]\n"
     ]
    }
   ],
   "source": [
    "# full example\n",
    "DimEmployee = sc.textFile(\"AdventureWorks/AdventureWorks_RDD/DimEmployee.csv\")\n",
    "FactResellerSales = sc.textFile(\n",
    "    \"AdventureWorks/AdventureWorks_RDD/FactResellerSales.csv\"\n",
    ")\n",
    "\n",
    "# split data\n",
    "DimEmployee = DimEmployee.map(lambda var1: var1.split(\",\"))\n",
    "FactResellerSales = FactResellerSales.map(lambda var1: var1.split(\",\"))\n",
    "\n",
    "# convert to key-value pairs\n",
    "DimEmployee = DimEmployee.map(lambda var1: [var1[0], [var1[1], var1[2], var1[3]]])\n",
    "FactResellerSales = FactResellerSales.map(\n",
    "    lambda var1: [\n",
    "        var1[5],\n",
    "        [\n",
    "            var1[0],\n",
    "            var1[1],\n",
    "            var1[2],\n",
    "            var1[3],\n",
    "            var1[4],\n",
    "            var1[5],\n",
    "            var1[6],\n",
    "            var1[7],\n",
    "            float(var1[8]),\n",
    "            float(var1[9]),\n",
    "            float(var1[10]),\n",
    "            float(var1[11]),\n",
    "            float(var1[12]),\n",
    "            float(var1[13]),\n",
    "            float(var1[14]),\n",
    "            float(var1[15]),\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# perform join\n",
    "RF = FactResellerSales.join(DimEmployee)\n",
    "RF.take(10)\n",
    "print(RF.map(lambda var1: (var1[0], var1[1][0])).take(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb318a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sepal.Length', 5.1), ('Sepal.Width', 3.5), ('Petal.Length', 1.4), ('Petal.Width', 0.2), ('Sepal.Length', 4.9), ('Sepal.Width', 3.0), ('Petal.Length', 1.4), ('Petal.Width', 0.2), ('Sepal.Length', 4.7), ('Sepal.Width', 3.2)]\n",
      "('Sepal.Length', 5.1)\n",
      "('Sepal.Width', 3.5)\n",
      "('Petal.Length', 1.4)\n",
      "('Petal.Width', 0.2)\n",
      "('Sepal.Length', 4.9)\n",
      "('Sepal.Width', 3.0)\n",
      "('Petal.Length', 1.4)\n",
      "('Petal.Width', 0.2)\n",
      "('Sepal.Length', 4.7)\n",
      "('Sepal.Width', 3.2)\n"
     ]
    }
   ],
   "source": [
    "# Loop through values\n",
    "iris1 = sc.textFile(\"iris/iris_site.csv\")\n",
    "iris1_split = iris1.map(lambda var1: var1.split(\",\"))\n",
    "iris1_mod = iris1_split.flatMap(\n",
    "    lambda var1: (\n",
    "        (\"Sepal.Length\", float(var1[0])),\n",
    "        (\"Sepal.Width\", float(var1[1])),\n",
    "        (\"Petal.Length\", float(var1[2])),\n",
    "        (\"Petal.Width\", float(var1[3])),\n",
    "    )\n",
    ")\n",
    "\n",
    "def fun(x): print(x)\n",
    "\n",
    "iris1_mod.foreach(fun)\n",
    "# print(iris1_mod.take(10))\n",
    "for data in iris1_mod.take(10):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59c77f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['7.1', '3.0', '5.9', '2.1', 'virginica'], ['7.6', '3.0', '6.6', '2.1', 'virginica'], ['7.3', '2.9', '6.3', '1.8', 'virginica'], ['7.2', '3.6', '6.1', '2.5', 'virginica'], ['7.7', '3.8', '6.7', '2.2', 'virginica'], ['7.7', '2.6', '6.9', '2.3', 'virginica'], ['7.7', '2.8', '6.7', '2.0', 'virginica'], ['7.2', '3.2', '6.0', '1.8', 'virginica'], ['7.2', '3.0', '5.8', '1.6', 'virginica'], ['7.4', '2.8', '6.1', '1.9', 'virginica']]\n"
     ]
    }
   ],
   "source": [
    "# filter data based on  a condition\n",
    "filter1=iris1_split.filter(lambda x:float(x[0])>7)\n",
    "print(filter1.take(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
