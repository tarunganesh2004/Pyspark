{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa28a14e",
   "metadata": {},
   "source": [
    "RDD -- Resilent distributed dataset\n",
    "\n",
    "RDD is the core low-level data structure in Spark that represents an immutable, distributed collection of objects processed in parallel.\n",
    "\n",
    "\n",
    "Letâ€™s break the word:\n",
    "\n",
    "Term\tMeaning\n",
    "\n",
    "Resilient\tFault-tolerant (recovers if a node fails)\n",
    "\n",
    "Distributed\tData split across multiple machines\n",
    "\n",
    "Dataset\tCollection of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e59afb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\hadoop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.environ.get(\"HADOOP_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a47f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age,income,gender,marital,buys', '24,130000,Female,Married,No', '23,140000,Female,Single,No', '27,150000,Female,Married,Yes', '51,70000,Female,Married,Yes', '53,50000,Male,Married,Yes', '56,40000,Male,Single,No', '29,30000,Male,Single,Yes', '21,80000,Female,Married,No', '19,20000,Male,Single,Yes', '61,90000,Male,Married,Yes', '17,100000,Male,Single,Yes', '28,110000,Female,Single,Yes', '31,160000,Male,Married,Yes', '55,120000,Female,Single,No']\n",
      "age,income,gender,marital,buys\n",
      "['age,income,gender,marital,buys', '24,130000,Female,Married,No', '23,140000,Female,Single,No', '27,150000,Female,Married,Yes', '51,70000,Female,Married,Yes', '53,50000,Male,Married,Yes', '56,40000,Male,Single,No', '29,30000,Male,Single,Yes', '21,80000,Female,Married,No', '19,20000,Male,Single,Yes']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext # this is old way of initializing spark \n",
    "# old core engine(works with RDD only )\n",
    "\n",
    "# create SparkContext\n",
    "sc = SparkContext(\"local\", \"ReadCSV\") # we can create only once\n",
    "\n",
    "# read file\n",
    "d1 = sc.textFile(\"iris/buy.csv\") # reading from a CSV file\n",
    "\n",
    "print(d1.collect())\n",
    "\n",
    "\n",
    "# print first line\n",
    "print(d1.first())\n",
    "\n",
    "# print only n values\n",
    "print(d1.take(10))\n",
    "\n",
    "sc.stop() # close the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188cb15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[2, 4, 6, 8, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new way --> spark Session(modern entry point)\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# multiline\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .appName(\"RDDFromZero\") \\\n",
    "#     .getOrCreate()\n",
    "# config is used to enable/disable features\n",
    "# appname is temporary (used for debugging ,job identification,logs)\n",
    "\n",
    "spark=SparkSession.builder.master(\"local[*]\").appName(\"RDDFromZero\").getOrCreate() \n",
    "# here master mentions where spark should run, local means on our own machine\n",
    "# we may not mention master(it defaultly takes local[*])\n",
    "\n",
    "# * means uses all cpu cores ,ex: local[2], spark stores our sparksession object\n",
    "\n",
    "sc=spark.sparkContext # sparkcontext is low level spark engine, used for RDD's \n",
    "\n",
    "# first RDD\n",
    "rdd=sc.parallelize([1,2,3,4,5]) # parallelize means split & distribute\n",
    "print(rdd.collect()) # collect--> brings all data from spark,converts to python list \n",
    "# rdd.count()\n",
    "# rdd.first()\n",
    "# rdd.take(3) # takes 1st 3 elements\n",
    "\n",
    "d1 = spark.sparkContext.parallelize([(1, 2), (2, 3), (3, 4), (4, 5), (5, 6)])\n",
    "# d1.collect()\n",
    "# d1.first()\n",
    "\n",
    "# Transformation\n",
    "rdd2=rdd.map(lambda x:x*2)\n",
    "print(rdd2.collect())\n",
    "\n",
    "# filter\n",
    "even=rdd.filter(lambda x:x%2==0)\n",
    "even.collect()\n",
    "\n",
    "# stop spark\n",
    "# spark.stop() # frees memory,stops background spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd959e91",
   "metadata": {},
   "source": [
    "Read from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15ddd7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sepal_Length,Sepal_Width,Petal_Length,Petal_Width,Species', '5.1,3.5,1.4,0.2,setosa', '4.9,3.0,1.4,0.2,setosa', '4.7,3.2,1.3,0.2,setosa', '4.6,3.1,1.5,0.2,setosa', '5.0,3.6,1.4,0.2,setosa', '5.4,3.9,1.7,0.4,setosa']\n",
      "[['5.1', '3.5', '1.4', '0.2', 'setosa'], ['4.9', '3.0', '1.4', '0.2', 'setosa'], ['4.7', '3.2', '1.3', '0.2', 'setosa'], ['4.6', '3.1', '1.5', '0.2', 'setosa'], ['5.0', '3.6', '1.4', '0.2', 'setosa'], ['5.4', '3.9', '1.7', '0.4', 'setosa'], ['4.6', '3.4', '1.4', '0.3', 'setosa'], ['5.0', '3.4', '1.5', '0.2', 'setosa'], ['4.4', '2.9', '1.4', '0.2', 'setosa'], ['4.9', '3.1', '1.5', '0.1', 'setosa']]\n",
      "['setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa']\n",
      "[3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1]\n",
      "[['5.1', '3.5', '1.4', '0.2', 'setosa'], ['4.7', '3.2', '1.3', '0.2', 'setosa'], ['4.6', '3.1', '1.5', '0.2', 'setosa'], ['5.0', '3.6', '1.4', '0.2', 'setosa'], ['5.4', '3.9', '1.7', '0.4', 'setosa'], ['4.6', '3.4', '1.4', '0.3', 'setosa'], ['5.0', '3.4', '1.5', '0.2', 'setosa'], ['4.9', '3.1', '1.5', '0.1', 'setosa'], ['5.4', '3.7', '1.5', '0.2', 'setosa'], ['4.8', '3.4', '1.6', '0.2', 'setosa']]\n"
     ]
    }
   ],
   "source": [
    "rdd=sc.textFile('iris/iris.csv') # reads file line by line \n",
    "print(rdd.take(7))\n",
    "\n",
    "# remove header\n",
    "header=rdd.first()\n",
    "data=rdd.filter(lambda line:line!=header)\n",
    "\n",
    "# split each line \n",
    "split_rdd=data.map(lambda line:line.split(\",\"))\n",
    "print(split_rdd.take(10))\n",
    "\n",
    "# accessing columns \n",
    "names=split_rdd.map(lambda row:row[4])\n",
    "print(names.take(10))\n",
    "\n",
    "# type conversion\n",
    "lengths=split_rdd.map(lambda row:float(row[1]))\n",
    "print(lengths.take(10))\n",
    "\n",
    "# filter data \n",
    "above3=split_rdd.filter(lambda row:float(row[1])>3)\n",
    "print(above3.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010c6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris/iris_site.csv\n",
      "import_RD\n"
     ]
    }
   ],
   "source": [
    "# RDD Metadata (info about how spark should handle that data)\n",
    "iris1=sc.textFile('iris/iris_site.csv')\n",
    "print(iris1.name())\n",
    "\n",
    "# set rdd name \n",
    "iris1.setName(\"import_RD\")\n",
    "print(iris1.name())\n",
    "\n",
    "rdd=sc.parallelize([1,2,3])\n",
    "print(rdd.id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store RDD data in server\n",
    "iris1=sc.textFile('iris/iris_site.csv')\n",
    "iris1.saveAsTextFile(\"./output/test1_csv\") # windows default native io issue, so using dataframes would be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4c13185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[44] at readRDDFromFile at PythonRDD.scala:299\n",
      "[('Sepal.Length', [5.1, 4.9, 4.7, 4.6]), ('Sepal.Width', [3.5, 3.0, 3.2, 3.1]), ('Species', ['setosa', 'setosa', 'setosa', 'setosa'])]\n",
      "{'Sepal.Length': [5.1, 4.9, 4.7, 4.6], 'Sepal.Width': [3.5, 3.0, 3.2, 3.1], 'Species': ['setosa', 'setosa', 'setosa', 'setosa']}\n",
      "[5.1, 4.9, 4.7, 4.6]\n",
      "['setosa', 'setosa', 'setosa', 'setosa']\n"
     ]
    }
   ],
   "source": [
    "# if the data is present as an RDD, it can be converted to python\n",
    "# dictionary object using collectAsMap function\n",
    "\n",
    "rdd1 = sc.parallelize(\n",
    "    [\n",
    "        (\"Sepal.Length\", [5.1, 4.9, 4.7, 4.6]),\n",
    "        (\"Sepal.Width\", [3.5, 3.0, 3.2, 3.1]),\n",
    "        (\"Species\", [\"setosa\", \"setosa\", \"setosa\", \"setosa\"]),\n",
    "    ]\n",
    ")\n",
    "print(rdd1)\n",
    "print(rdd1.collect())\n",
    "\n",
    "# convert to dict \n",
    "dict1=rdd1.collectAsMap()\n",
    "print(dict1)\n",
    "print(dict1['Sepal.Length'])\n",
    "print(dict1['Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5b3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+\n",
      "|col1|col2|col3| col4|\n",
      "+----+----+----+-----+\n",
      "|   1|   2|   3|a b c|\n",
      "|   4|   5|   6|d e f|\n",
      "|   7|   8|   9|g h i|\n",
      "+----+----+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(col1=1, col2=2, col3=3, col4='a b c'),\n",
       " Row(col1=4, col2=5, col3=6, col4='d e f'),\n",
       " Row(col1=7, col2=8, col3=9, col4='g h i')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # creating a rdd\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark=SparkSession.builder.appName(\"Python Spark create RDD example\").config(\"random-config\",\"some-value\").getOrCreate()\n",
    "\n",
    "# df=spark.sparkContext.parallelize([(1,2,3,'a b c'),\n",
    "#                                   (4,5,6,'d e f'),\n",
    "#                                   (7,8,9,'g h i')]).toDF(['col1','col2','col3','col4'])\n",
    "\n",
    "# df.show()\n",
    "\n",
    "# df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6a970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n",
      "| ID| Name|Salary|DeptId|\n",
      "+---+-----+------+------+\n",
      "|  1|  Joe| 70000|     1|\n",
      "|  2|Henry| 80000|     2|\n",
      "|  3|  Sam| 60000|     1|\n",
      "|  4|  Max| 90000|     1|\n",
      "+---+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# d2=spark.createDataFrame([(1,'Joe','70000','1'),\n",
    "#                           ('2','Henry','80000','2'),\n",
    "#                           ('3','Sam','60000','1'),\n",
    "#                           ('4','Max','90000','1')],\n",
    "#                           ['ID','Name','Salary','DeptId'])\n",
    "\n",
    "# d2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
